{"guide": {"name": "messages-format", "category": "chatbots", "pretty_category": "Chatbots", "guide_index": 3, "absolute_index": 25, "pretty_name": "Messages Format", "content": "# Using the Messages data format\n\nIn the previous guides, we built chatbots where the conversation history was stored in a list of tuple pairs.\nIt is also possible to use the more flexible [Messages API](https://huggingface.co/docs/text-generation-inference/en/messages_api#messages-api), which is fully compatible with LLM API providers such as Hugging Face Text Generation Inference, Llama.cpp server, and OpenAI's chat completions API.\n\nTo use this format, set the `type` parameter of `gr.Chatbot` or `gr.ChatInterface` to `'messages'`. This expects a list of dictionaries with content and role keys.\n\nThe `role` key should be `'assistant'` for the bot/llm and `user` for the human.\n\nThe `content` key can be one of three things:\n\n1. A string (markdown supported) to display a simple text message\n2. A dictionary (or `gr.FileData`) to display a file. At minimum this dictionary should contain a `path` key corresponding to the path to the file. Full documenation of this dictionary is in the appendix of this guide.\n3. A gradio component - at present `gr.Plot`, `gr.Image`, `gr.Gallery`, `gr.Video`, `gr.Audio` are supported. \n\nFor better type hinting and auto-completion in your IDE, you can use the `gr.ChatMessage` dataclass:\n\n```python\nfrom gradio import ChatMessage\n\ndef chat_function(message, history):\n    history.append(ChatMessage(role=\"user\", content=message))\n    history.append(ChatMessage(role=\"assistant\", content=\"Hello, how can I help you?\"))\n    return history\n```\n\n## Examples\n\nThe following chatbot will always greet the user with \"Hello\"\n\n```python\nimport gradio as gr\n\ndef chat_greeter(msg, history):\n    history.append({\"role\": \"assistant\", \"content\": \"Hello!\"})\n    return history\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot(type=\"messages\")\n    msg = gr.Textbox()\n    clear = gr.ClearButton([msg, chatbot])\n\n    msg.submit(chat_greeter, [msg, chatbot], [chatbot])\n\ndemo.launch()\n```\n\nThe messages format lets us seemlessly stream from the Hugging Face Inference API -\n\n```python\nimport gradio as gr\nfrom huggingface_hub import InferenceClient\n\nclient = InferenceClient(\"HuggingFaceH4/zephyr-7b-beta\")\n\ndef respond(message, history: list[dict]):\n\n    messages = history + [{\"role\": \"user\", \"content\": message}]\n\n    print(messages)\n\n    response = {\"role\": \"assistant\", \"content\": \"\"}\n\n    for message in client.chat_completion(\n        messages,\n        max_tokens=512,\n        stream=True,\n        temperature=0.7,\n        top_p=0.95,\n    ):\n        token = message.choices[0].delta.content\n\n        response['content'] += token\n        yield response\n\n\ndemo = gr.ChatInterface(respond, type=\"messages\")\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n\n### Appendix\n\nThe full contents of the dictionary format for files is documented here\n\n```python\nclass FileDataDict(TypedDict):\n    path: str  # server filepath\n    url: NotRequired[Optional[str]]  # normalised server url\n    size: NotRequired[Optional[int]]  # size in bytes\n    orig_name: NotRequired[Optional[str]]  # original filename\n    mime_type: NotRequired[Optional[str]]\n    is_stream: NotRequired[bool]\n    meta: dict[Literal[\"_type\"], Literal[\"gradio.FileData\"]]\n```", "tags": [], "spaces": [], "url": "/guides/messages-format/", "contributor": null}}